{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e72c0372",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<b><font size=\"6\">Deep Learning Project: Identifying Musical Instruments</font></b>\n",
    "\n",
    "**Submitted by:**\n",
    "\n",
    "    Elias Assaf \n",
    "    \n",
    "    Ghada Abu Elzalaf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b02317",
   "metadata": {},
   "source": [
    "<!-- vscode-markdown-toc -->\n",
    "\n",
    "<!-- vscode-markdown-toc-config\n",
    "\tnumbering=true\n",
    "\tautoSave=true\n",
    "\t/vscode-markdown-toc-config -->\n",
    "<!-- /vscode-markdown-toc -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc90aed3-ef36-4c65-b92f-e5c9ef3b6597",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "In the field of music information retrieval (MIR), recognizing the musical instruments played within an audio clip is crucial. It enables music search by specific instruments, computes the similarity between compositions and helps recognize musical genres. <br>\n",
    "In our project we propose 3 different neural network architectures to solve the task, each with it's own properties, CNN's that extract high level features of the input, RNN's that consider temporal properties of the input, and finally using attention models so we can set the model to focus on specific parts of the input, they are trained with single instrument audio clips and tested with polyphonic music with several instruments. <br>\n",
    "The data is given in a .wav format which are uncompressed waveforms of the signal that come in 2 channels (stereo), in the project we will explore different transformations to apply on the data such as the stft and mel-spectrogram, to get the best possible output results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e7e67c-09cf-43f6-932a-7e20f8976293",
   "metadata": {},
   "source": [
    "<b><font size=\"4\">Environment setup</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d36d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#requires python version >= 3.8\n",
    "\n",
    "import torch\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnF\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from scipy.io import wavfile\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import time\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import gc\n",
    "\n",
    "import wget\n",
    "\n",
    "import os\n",
    "\n",
    "from torch.multiprocessing import Manager\n",
    "\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "\n",
    "# !pip install tabulate\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8523bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "SAMPLING_RATE = 44100\n",
    "N_FFT = 4096\n",
    "N_MELS = 224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96130a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset download \n",
    "\n",
    "data_path = \"./data\" \n",
    "\n",
    "base_url = \"https://zenodo.org/record/1290750/files\"\n",
    "\n",
    "file_list = [\n",
    "    \"IRMAS-TrainingData.zip\",\n",
    "    \"IRMAS-TestingData-Part1.zip\",\n",
    "    \"IRMAS-TestingData-Part2.zip\",\n",
    "    \"IRMAS-TestingData-Part3.zip\",\n",
    "]\n",
    "\n",
    "# Path to folder with the dataset\n",
    "dataset_folder = f\"{data_path}/irmas\"\n",
    "os.makedirs(dataset_folder, exist_ok=True)\n",
    "\n",
    "for file in tqdm(file_list):\n",
    "    url = f\"{base_url}/{file}\"\n",
    "    if not os.path.exists(f\"{dataset_folder}/{file}\"):\n",
    "        wget.download(url, f\"{dataset_folder}/{file}\")\n",
    "for file in tqdm(file_list):\n",
    "    if not os.path.exists(f\"{dataset_folder}/{file.split('.')[0]}\"):\n",
    "        with zipfile.ZipFile(f\"{dataset_folder}/{file}\", \"r\") as ziphandler:\n",
    "            ziphandler.extractall(dataset_folder)\n",
    "print('done')\n",
    "trainingSet_folder = f\"{dataset_folder}/IRMAS-TrainingData\"\n",
    "testingSet_folder = f\"{dataset_folder}/IRMAS-TestingData\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53711d0a-e7ab-4ab8-b6cf-52973352be6d",
   "metadata": {},
   "source": [
    "# Dataset and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f1f2a7-ac85-419b-b52f-3841f41426de",
   "metadata": {},
   "source": [
    "## Data set\n",
    "\n",
    "IRMAS dataset which includes musical audio files with metadata files that describe the prominent instrument in each\n",
    "recording.\n",
    "\n",
    "https://www.upf.edu/web/mtg/irmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e835fc0c-9cbc-4988-80e2-eabd12cf3854",
   "metadata": {},
   "source": [
    "### Training data\n",
    "\n",
    "6705 audio files in 16 bit stereo wav format sampled at 44.1kHz. where each file is a 3 second signal from 2000 different recordings.\n",
    "\n",
    "The instruments are:\n",
    "- cello - cel(388)\n",
    "- clarinet - cla(505)\n",
    "- flute - flu(451)\n",
    "- guitar - gac(637)\n",
    "- guitar - gel(760)\n",
    "- organ - org(682)\n",
    "- piano - pia(721)\n",
    "- saxophone - sax(626)\n",
    "- trumpet - tru(577)\n",
    "- violin - vio(580)\n",
    "- human singing voice - voi(778)\n",
    "\n",
    "where each number inside the parentheses describe the number of audio files that contain that specific instrument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df56bf-0a3a-42c1-83bc-c73bf6383992",
   "metadata": {},
   "source": [
    "### Test data\n",
    "\n",
    "2874 files in 16 bit stereo wav format sampled at 44.1kHz.\n",
    "Each file ranges from 5-20 seconds long, where each files has 1 or more instrument.\n",
    "\n",
    "Each music file is accompanied by a text file that has 1 instrument annotation per line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b033aa17-da05-4e2f-83d3-003ae0e6f17b",
   "metadata": {},
   "source": [
    "## Data Set Class\n",
    "We build a Dataset class to help us extract the data sampels easily and apply transforms on them.\n",
    "\n",
    "We will describe the functionality and purpose of each transformation later on, as to show illustrative figures along with the explanation.\n",
    "\n",
    "Note: One of the problems that we had was that applying our chosen transformations took a long time and was a bottleneck in the training process, where each epoch took around 2 mins to finish, to overcome this problem we added an option for caching the results (while also keeping in mind random transformations), we have around 1700 files for the training data where each result after the transform is an image of size about 224x224 or 2x224x224, all in all, this results in around 100 Mb of storage which isn't much and so we can easily keep all the results in ram, to maximize the throughput we also use a parallel dictionary (from multiprocessing package using Manager) so we can reuse the same dataset for multiple models and multiple data loader processes.<br>\n",
    "All of these optimizations resulted in a training time of about 2-3 seconds per epoch (down from 2 mins) for some of our models (on i5-8600K and GTX 1070 ti)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b9cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRMASDataset(Dataset): \n",
    "    def __init__(self, wav_dir, split='train', mono=False, cache=False, transform=None, target_transform=None, random_transform=None):\n",
    "        # the class gets -wav_dir- which is the file path, -split- for categorizing the data i.e. 'train', 'valid' or 'test'\n",
    "        # -mono- if true gets the mono file else gets the stereo, -cache- if true caches the data after loading, -transform- is the transform \n",
    "        # to apply on the audio data, -traget_transform- is the transform to apply on the labels.\n",
    "        if split == 'train':\n",
    "            self.paths = glob.glob(wav_dir + \"/*/*.wav\")\n",
    "            self.labels = [path.split('/')[4] for path in self.paths]\n",
    "        elif split == 'valid':\n",
    "            self.paths = glob.glob(wav_dir + \"/*/*.wav\")[0::10]\n",
    "            self.labels = [path.split('/')[4] for path in self.paths]\n",
    "        elif split == 'test':\n",
    "            self.paths = glob.glob(wav_dir + \"/*.wav\")\n",
    "            label_paths = [path.replace(\".wav\", \".txt\") for path in self.paths]\n",
    "            translator = str.maketrans({'\\t': '', '\\n': '', ' ': ''})\n",
    "            self.labels = [[s.translate(translator) for s in open(path, 'r').readlines()]  for path in label_paths]\n",
    "        else:\n",
    "            raise Exception(\"split must be 'train' or 'test' or 'valid'\")\n",
    "            \n",
    "        self.mono = mono            \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.random_transform = random_transform\n",
    "        self.cache = cache\n",
    "        if(cache):\n",
    "            self.manager = Manager()\n",
    "            self._dict = self.manager.dict()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # if isinstance(idx, slice):\n",
    "        #     return self.getSlice(idx.start, idx.stop, idx.step)\n",
    "        \n",
    "        if self.cache and (to_ret := self._dict.get(idx)): # if cache is true then it checks wether the data in the specified index is cached if not it loads it and caches it.\n",
    "            return (self.random_transform(to_ret[0]), to_ret[1]) if self.random_transform else to_ret\n",
    "        \n",
    "        wave, _ = librosa.load(self.paths[idx], sr=None, mono=self.mono)\n",
    "        wave = wave / wave.max()\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            wave = self.transform(wave)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        if(self.cache):\n",
    "            self._dict[idx] = (wave, label)\n",
    "        if self.random_transform:\n",
    "            wave = self.random_transform(wave)\n",
    "        return wave, label\n",
    "    \n",
    "    def clear(self):\n",
    "        if self.cache:\n",
    "            self._dict.clear()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3199bd-db52-4ad0-a21a-c3abdce87ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = IRMASDataset(trainingSet_folder)\n",
    "test_dataset[5][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a587c7-0029-4ea7-9ace-a98647901b8f",
   "metadata": {},
   "source": [
    "## Data Preperation - Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905f761b-152a-4228-a5d4-cd60404e09f8",
   "metadata": {},
   "source": [
    "### One-hot Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13756756-723d-4aa4-816e-937115ee73b3",
   "metadata": {},
   "source": [
    "Our task is a classification problem, so one necessary transform is the one-hot representation of the labels to enable the calculation of the loss function. We extract the unique labels and build a transform class (input for target_transform in the dataset class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3873ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class one_hot_transform(object):\n",
    "    def __init__(self, path=trainingSet_folder, testing_data = False):\n",
    "        instruments = [f.split('/')[-1] for f in glob.glob(path + \"/*\")]\n",
    "        self.nti = {f:i for i,f in enumerate(instruments)}\n",
    "        self.itn = {i:f for i,f in enumerate(instruments)}\n",
    "        self.n = len(instruments)\n",
    "        self.eye = np.eye(self.n, dtype='float32')\n",
    "        self.testing_data = testing_data\n",
    "    \n",
    "    #check dimensional representation\n",
    "    def __call__(self, labels):\n",
    "        \n",
    "        if(isinstance(labels, list)):\n",
    "            idxs = [self.nti[label] for label in labels]\n",
    "        else:\n",
    "            idxs = self.nti[labels]\n",
    "        return torch.tensor(idxs, dtype=torch.long) if self.testing_data else self.eye[idxs]\n",
    "    \n",
    "    # returns the names of the predected instruments\n",
    "    def inverse(self, preds):\n",
    "        if(isinstance(preds, list)):\n",
    "            names = [self.itn[pred] for pred in preds]\n",
    "        elif(torch.is_tensor(preds)):\n",
    "            names = [self.itn[int(pred)] for pred in preds]\n",
    "        else:\n",
    "            names = self.itn[preds]\n",
    "        return names\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f6771d-36d4-4623-8044-c5c1d816e18e",
   "metadata": {},
   "source": [
    "And obviously sanity checking it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b374c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = one_hot_transform()\n",
    "print(\"One hot representation of cello and saxophone:\\n{}\\n\\nInverse transform of some random labels: {}\\n{}\\n\".format(one_hot([\"cel\", \"sax\"]), [0,2,6], one_hot.inverse([0,2,6])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639c969f-933a-4e22-8793-f713a700ab55",
   "metadata": {},
   "source": [
    "### Mono-Stereo Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cb4f1d-fa1c-4ec1-a535-618ed6881978",
   "metadata": {},
   "source": [
    "We also add a quick transform that transforms the audio from stereo to mono:\n",
    "\n",
    "(for the training in the end we added an option for the dataset input to read as mono since we get a higher throughput reading the data from the start as mono)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ae83eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mono_transform(object):\n",
    "    def __init(self):\n",
    "        pass\n",
    "    def __call__(self, wave):\n",
    "        return librosa.to_mono(wave)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b424d1d-4d96-4b1b-a91d-34713e4cb345",
   "metadata": {},
   "source": [
    "Sanity checks for the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77814b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = IRMASDataset(wav_dir=trainingSet_folder, split='train', target_transform = one_hot_transform())\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size=64, shuffle=True)#, num_workers=1, prefetch_factor=4)\n",
    "\n",
    "test_data = IRMASDataset(wav_dir=testingSet_folder, split='test', target_transform = one_hot_transform(testing_data=True))\n",
    "#data loader can only do 1 batch a time because each sample has a different length\n",
    "test_data_loader = DataLoader(test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3952a058-3e63-4649-8441-6dea3cdc2728",
   "metadata": {},
   "source": [
    "Stereo vs mono audio sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a478907",
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_trans = mono_transform()\n",
    "train_iter = iter(train_data_loader)\n",
    "train_features, train_labels = next(train_iter)\n",
    "train_feature_example = train_features[0].numpy()\n",
    "train_feature_example_mono = mono_trans(train_feature_example)\n",
    "print(\"Dual channel size: {}\".format(train_feature_example.shape))\n",
    "print(\"Mono channel size: {}\".format(train_feature_example_mono.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701bc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epo = iter(test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024d1711-8712-4409-ba9c-63e5f8629eec",
   "metadata": {},
   "source": [
    "How a batch with no transformations looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c850ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print(next(train_iter)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c610c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Batch Example: {}\".format(train_features.size()))\n",
    "\n",
    "librosa.display.waveshow(train_features[0].numpy(), sr=SAMPLING_RATE)\n",
    "plt.title(\"a waveform in time visualization\")\n",
    "plt.gcf().set_size_inches(6,2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f573f10",
   "metadata": {},
   "source": [
    "### Short Time Fourier Transform (STFT)\n",
    "\n",
    "The Short Time Fourier Transform (STFT) is a way to look at natural audio signals, it is defined as taking the DFT over windows on the signal, using short windows $20-30 ms$ for speech and $\\sim 90ms$ for musical signals, we can treat the signals as stationary, while keeping a good spectral resolution that has valuable information on the underlying signal.\n",
    "\n",
    "In our signals we take the window length to be approximately 93 ms, This corresponds to 4101 samples per window which we round to 4096 for fft effciency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e0ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_example = np.abs(librosa.stft(train_feature_example, n_fft=N_FFT))\n",
    "\n",
    "print(\"The stft dimensions for a dual channel audio at 4096 samples per window: {}\\nIn our training data there is 130 windows per sample\".format(stft_example.shape))\n",
    "plt.plot(stft_example[0,:,0:4])\n",
    "plt.gcf().set_size_inches(6,2.5)\n",
    "plt.title(\"FFT of multiple windows example\")\n",
    "plt.legend([\"window {} fft\".format(i) for i in range(4)])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1126ed15-d1a3-4e34-95fd-f356c8981d1f",
   "metadata": {},
   "source": [
    "interestingly the data has 0 values at frequencies larger than about 500, that means that most of our data is useless. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764753c5",
   "metadata": {},
   "source": [
    "### Spectogram\n",
    "\n",
    "Another way to show STFT is to use spectrograms, which model each window as a column, and all the FFT of the windows as an image, in other words a time-frequency relation, these are especially useful because they represent sounds as a 2-d image which allow the use of CNN's ;). <br>\n",
    "We choose to use the melspectrogram because it extracts better features than the normal spectrogram since it converts the frequency axis from linear Hertz scale to a non-linear mel scale, which more closely approximates the way human ears perceive pitch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c3d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of mel spectogram\n",
    "stft_example_mono = np.abs(librosa.stft(train_feature_example_mono, n_fft=N_FFT))\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "stft_example_mono_db = librosa.amplitude_to_db(stft_example_mono, ref=np.max)\n",
    "print(stft_example_mono_db.shape)\n",
    "img = librosa.display.specshow(stft_example_mono_db, x_axis='time', y_axis='log', sr=SAMPLING_RATE)\n",
    "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "\n",
    "ax.set(title='Spectogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0a927b",
   "metadata": {},
   "source": [
    "We show the spectogram for the mono audio as to not show 2 graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c47851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_example = librosa.feature.melspectrogram(y=train_feature_example_mono, sr=SAMPLING_RATE, n_fft=N_FFT, n_mels=N_MELS)\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "spectrogram_example_db = librosa.power_to_db(spectrogram_example, ref=np.max)\n",
    "img = librosa.display.specshow(spectrogram_example_db, x_axis='time',y_axis='mel', sr=SAMPLING_RATE)\n",
    "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "ax.set(title='Mel-frequency Spectrogram')\n",
    "plt.show()\n",
    "print(spectrogram_example.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bfbffd-0bdf-4c84-b530-14384ee622d3",
   "metadata": {},
   "source": [
    "We can see that in the mel-frequency spectogram we get a smoother graph that all the the difference in frequency values are equal, for example in the hz scale we can clearly hear the difference between a 200hz to 400hz signal, but we cannot hear the difference in 8000hz and 8200hz, on the other hand in the mel-scale a 200 difference in the low range is equivalent to a 200 difference in a higher range.\n",
    "\n",
    "We also notice the difference in the shape of the mel-frequency spectogram (224,259) and the regular spectrogram (2049, 130), which sort of compresses the data from the full spectogram to a smaller space, that keeps the important features, and at the same time we get rid of the many zeros in the stft which gives us a lighter network.\n",
    "\n",
    "We can also control the variable n_mels to change the number of mel-frequencies that we want to calculate, this helps us get spectograms of desired shapes that better fit our network architecture, in the example above we picked this number to be n_mels=224, that is a good value because it can be divided by 2 many times, and we also used a prebuilt network that needed 224x224 input shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ec2be8-a84c-49ff-83c1-211df28d6763",
   "metadata": {},
   "source": [
    "And so we define the mel-frequency spectogram transform that we can use in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed09c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mel_frequency_transform(object):\n",
    "    def __init__(self, sr=SAMPLING_RATE, n_fft=N_FFT, n_mels=N_MELS):\n",
    "        self.sr = SAMPLING_RATE\n",
    "        self.n_fft = n_fft\n",
    "        self.n_mels = n_mels\n",
    "        \n",
    "    def __call__(self, wave):\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=wave, sr=self.sr, n_fft=self.n_fft, n_mels=self.n_mels)\n",
    "        return librosa.power_to_db(mel_spectrogram, ref=np.max)[...,1:1+256]\n",
    "        #we cut the signal to reach length of 256 instead of 259\n",
    "\n",
    "class spectogram_transform(object):\n",
    "    def __init__(self, n_fft=N_FFT):\n",
    "        self.n_fft = n_fft\n",
    "        \n",
    "    def __call__(self, wave):\n",
    "        spectogram = np.abs(librosa.stft(wave, n_fft=N_FFT))\n",
    "        return librosa.amplitude_to_db(spectogram, ref=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da39164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check\n",
    "mel_transform = mel_frequency_transform()\n",
    "mel_example = mel_transform(train_feature_example)\n",
    "print(\"stft shape: {}, mel shape: {}, mel size in Kilobytes: {}\\n\".format(stft_example.shape, mel_example.shape, mel_example.size * mel_example.itemsize//8000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da51004d",
   "metadata": {},
   "source": [
    "### Zero Crossing Rate - ZCR\n",
    "\n",
    "One more metric that can help is the zero crossing rate, that is defined as the rate at which a signal changes from positive to zero to negative or from negative to zero to positive and is given by:\n",
    "Where is the signal and\n",
    "\n",
    "is its length.\n",
    "\n",
    "The ZCR gives us more information about the underlying signal that the neural network can utilize, we can add it as a channel next to the mel_transform.\n",
    "\n",
    "We replace the last coloumn of the Mel spectogram with the ZCR to add more information to the data. We crop the spectogram such that we get dimentions that are multiplies of 2, which would make it easier for us to lower the dimintionality of the data in the CNN.\n",
    "\n",
    "**Important:** as we found out experimenting, the ZCR gave worse results overall, so we didn't include it's results, but kept it here so we can discuss it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8fd06c-408a-45c7-8a25-ff54047f2a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "zcr_example = librosa.feature.zero_crossing_rate(train_feature_example_mono, frame_length=N_FFT)\n",
    "plt.plot(3*np.arange(zcr_example.size)/zcr_example.size, zcr_example.T)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"ZCR\")\n",
    "plt.show()\n",
    "print(zcr_example.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0430ae-f39c-4216-98a5-166494d671d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class mel_frequency_ZCR_transform(object):\n",
    "    def __init__(self, sr=SAMPLING_RATE, n_fft=N_FFT, n_mels=N_MELS):\n",
    "        self.sr = SAMPLING_RATE\n",
    "        self.n_fft = n_fft\n",
    "        self.n_mels = n_mels\n",
    "        \n",
    "    def __call__(self, wave):\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=wave, sr=self.sr, n_fft=self.n_fft, n_mels=self.n_mels)\n",
    "        mel = librosa.power_to_db(mel_spectrogram, ref=np.max)[...,:-1,1:1+256]\n",
    "        zcr = librosa.feature.zero_crossing_rate(wave, frame_length=self.n_fft)[..., 1:1+256]\n",
    "        ax = 1 if wave.shape[0] == 2 else 0\n",
    "        return np.concatenate((mel,zcr), axis=ax).astype('float32')\n",
    "\n",
    "        #we cut the signal to reach length of 256 instead of 259\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ec165-f740-4362-bf56-51bfa08b1a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Sanity check\n",
    "mel_ZCR_transform = mel_frequency_ZCR_transform()\n",
    "mel_ZCR_example = mel_ZCR_transform(train_feature_example)\n",
    "mel_ZCR_example_mono = mel_ZCR_transform(train_feature_example_mono)\n",
    "print(\"mel+ZCR shape stereo: {}\\nmel+ZCR shape mono: {}\\n\".format(mel_ZCR_example.shape, mel_ZCR_example_mono.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d9ee5-f44a-454c-b57b-e114896a85da",
   "metadata": {},
   "source": [
    "\n",
    "Zero Crossing Rate - ZCR\n",
    "\n",
    "One more metric that can help is the zero crossing rate, that is defined as the rate at which a signal changes from positive to zero to negative or from negative to zero to positive and is given by:\n",
    "Where is the signal and\n",
    "\n",
    "is its length.\n",
    "\n",
    "The ZCR gives us more information about the underlying signal that the neural network can utilize, we can add it as a channel next to the mel_transform.\n",
    "\n",
    "We replace the last coloumn of the Mel spectogram with the ZCR to add more information to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa40c3cc",
   "metadata": {},
   "source": [
    "# Model architectures:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39f408a",
   "metadata": {},
   "source": [
    "We propose 3 models to deal with project purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f2a3e1-1acc-41e9-bedf-5d34f9bf9c21",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adae3569-50e8-481f-8236-74b1167d08c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321d84b1-21c5-4f87-87b4-68fa2abfced6",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "This model is based on 4 layers of CNN followed by 2 fully connected layers, it gets as input mel spectrogram of an audio clip, and classifies the musical instruments that are present in the audio.<br>\n",
    "Since the model is based on CNN we can easily change the number of input channels, therefore we can feed it mel spectrogram of one or two channels that represent the mono or stereo audio. \n",
    "Also, to experiment the model on variant relu activation functions, we added a parameter to the model that determines the activation function i.e relu with a threshold. We tried Relu and LeakyRelu with threshold = 0.3.\n",
    "\n",
    "The model consists of blocks as follows: \n",
    "\n",
    "**Block 1:**<br>\n",
    "Convolutional layer with $m$ input channels and outputs $n$ channels, the kernel size is $3x3$ and stride $2x2$.<br>\n",
    "Batch normalization is applied and relu is activated.\n",
    "\n",
    "**Block 2:**<br>\n",
    "Convolutional layer with $n$ input channels and outputs $2 \\cdot n$ channels, the kernel size is $3x3$ and stride $2x2$.<br>\n",
    "Batch normalization is applied and relu is activated.\n",
    "\n",
    "**Block 3:**<br>\n",
    "Convolutional layer with $2 \\cdot n$ input channels and outputs $4 \\cdot n$ channels, the kernel size is $3x3$ and stride $2x2$.<br>\n",
    "Batch normalization is applied and relu is activated.\n",
    "\n",
    "**Block 4:**<br>\n",
    "Convolutional layer with $4 \\cdot n$ input channels and outputs $8 \\cdot n$ channels, the kernel size is $3x3$ and stride $2x2$.<br>\n",
    "Batch normalization is applied and relu is activated.\n",
    "\n",
    "**Fully connected 0:**<br> \n",
    "Fully connected layer followed by relu activation function.\n",
    "\n",
    "**Fully connected 1:**<br>\n",
    "Fully connected layer that outputs one-hot vector representing the classification of the played instruments.\n",
    "\n",
    "We train the model with $m$ input channels which gets 1 or 2 based on the audio channels, i.e. for mono channel $m=1$ and for stereo $m=2$, and $n$ number of hidden channels, where we trained the model once with $n=16$ and another with $n=32$. In addition, we set in_size as 224x256 which represents the size of the mel spectrogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b985aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIMPLE_CNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, latent, in_size, relu_type=\"relu\", neg_slope=0.01):\n",
    "        super(SIMPLE_CNN, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.latent = latent\n",
    "        self.in_size = in_size\n",
    "        \n",
    "        # Using different activation functions for comparision, we used rely and leaky relu with negative slope parameter \n",
    "        if relu_type == \"relu\":\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "        elif relu_type == \"leaky_relu\":\n",
    "            self.relu = nn.LeakyReLU(negative_slope=neg_slope, inplace=True)\n",
    "        else: \n",
    "            raise Exception(\"relu_type must be 'relu' or 'leaky_relu'\")\n",
    "                \n",
    "        # self.relu = nn.ReLU(inplace=True) if relu_typr == \"relu\" else nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        self.block1 = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=num_hiddens, kernel_size=(3,3), stride=(2,2), padding=1),\n",
    "                                    nn.BatchNorm2d(num_hiddens),\n",
    "                                    self.relu)\n",
    "        self.block2 = nn.Sequential(nn.Conv2d(in_channels=num_hiddens, out_channels=2*num_hiddens, kernel_size=(3,3), stride=(2,2), padding=1),\n",
    "                                    nn.BatchNorm2d(2*num_hiddens),\n",
    "                                    self.relu)\n",
    "\n",
    "        self.block3 = nn.Sequential(nn.Conv2d(in_channels=2*num_hiddens, out_channels=2*2*num_hiddens, kernel_size=(3,3), stride=(2,2), padding=1),\n",
    "                                    nn.BatchNorm2d(2*2*num_hiddens),\n",
    "                                    self.relu)\n",
    "        \n",
    "        self.block4 = nn.Sequential(nn.Conv2d(in_channels=2*2*num_hiddens, out_channels=2*2*2*num_hiddens, kernel_size=(3,3), stride=(2,2), padding=1),\n",
    "                                    nn.BatchNorm2d(2*2*2*num_hiddens),\n",
    "                                    self.relu)\n",
    "\n",
    "        self.fc0 = nn.Linear(2*2*2*num_hiddens * self.in_size[0]//16 * self.in_size[1]//16, 8*latent)\n",
    "        self.fc1 = nn.Linear(8*latent, latent)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.block1(inputs)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = x.view(inputs.shape[0], 2*2*2*self.num_hiddens * self.in_size[0]//16 * self.in_size[1]//16)\n",
    "        x = self.relu(self.fc0(x))\n",
    "        return self.fc1(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f623c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity Check:\n",
    "#build a batch of size 2 and run it through the network\n",
    "test_net = SIMPLE_CNN(1, 16, 11, (224,256))\n",
    "test_input = transforms.ToTensor()(mel_transform(train_feature_example_mono))\n",
    "test_input = torch.stack((test_input,test_input))\n",
    "\n",
    "x = test_net(test_input)\n",
    "print(\"input shape: {}, output shape: {}\\n\".format(test_input.shape, x.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab8c55-cbb8-49a9-9561-464f0d419350",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8c6ced-821f-4ab3-a0d9-04af86e5bf3e",
   "metadata": {},
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e573138-d7de-4406-a39f-684dd2b869e2",
   "metadata": {},
   "source": [
    "\n",
    "This model is based on 4 layeres of CNN followed by a RNN layer and 2 fully connected layers, that gets as input mel spectogram of an audio clip, and classifies the musical instruments that are present in the audio.<br>\n",
    "Since the model is based on CNN we can easily change the size of the input, therefore we can feed it mel spectrogram of one or two channels that represent the mono or stereo audio. \n",
    "Also, to experiment the model on variant relu activation functions, we added a parameter to the model that determines the activation function i.e relu with a threshold. We tried Relu and LeakyRelu with threshold = 0.3.\n",
    "\n",
    "The model consists of blocks as follows: \n",
    "\n",
    "**Block 1:**<br>\n",
    "Convolutional layer with $m$ input channels and outputs $n$ channels, the kernel size is $3x3$ and stride $2x2$.<br>\n",
    "Batch normalization is applied and relu is activated.\n",
    "\n",
    "**Block 2:**<br>\n",
    "Convolutional layer with $n$ input channels and outputs $2 \\cdot n$ channels, the kernel size is $3x3$ and stride $2x2$.<br>\n",
    "Batch normalization is applied and relu is activated.\n",
    "\n",
    "**Block 3:**<br>\n",
    "Convolutional layer with $2 \\cdot n$ input channels and outputs $4 \\cdot n$ channels, the kernel size is $3x3$ and stride $2x2$.<br>\n",
    "Batch normalization is applied and relu is activated.\n",
    "\n",
    "**Block 4:**<br>\n",
    "Convolutional layer with $4 \\cdot n$ input channels and outputs $8 \\cdot n$ channels, the kernel size is $3x3$ and stride $2x2$.<br>\n",
    "Batch normalization is applied and relu is activated.\n",
    "\n",
    "**RNN layer:**<br> \n",
    "RNN layer that outputs $8 \\cdot n$ channels that gets as an input the rearranged output of block 4.\n",
    "\n",
    "**Fully connected 0:**<br> \n",
    "Fully connected layer followed by relu activation function.\n",
    "\n",
    "**Fully connected 1:**<br>\n",
    "Fully connected layer that outputs one-hot vector representing the classification of the played instruments.\n",
    "\n",
    "We train the model with $m$ input channels which gets 1 or 2 based on the audio channels, i.e. for mono channel $m=1$ and for stereo $m=2$, and $n$ number of hidden channels, where we trained the model once with $n=16$ and another with $n=32$. In addition, we set in_size as 224x256 which represents the size of the mel spectrogram.\n",
    "\n",
    "We chose the \"time\" sequence of the RNN to be the height channel, we didn't test any other channel because we got good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca2b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_RNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, latent, in_size, relu_type=\"relu\", neg_slope=0.01):\n",
    "        super(CNN_RNN, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.latent = latent\n",
    "        self.in_size = in_size\n",
    "        \n",
    "        # Using different activation functions for comparision, we used rely and leaky relu with negative slope parameter \n",
    "        if relu_type == \"relu\":\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "        elif relu_type == \"leaky_relu\":\n",
    "            self.relu = nn.LeakyReLU(negative_slope=neg_slope, inplace=True)\n",
    "        else: \n",
    "            raise Exception(\"relu_type must be 'relu' or 'leaky_relu'\")\n",
    "            \n",
    "        self.block1 = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=num_hiddens, kernel_size=(3,3), stride=(2,2), padding=1),\n",
    "                                    nn.BatchNorm2d(num_hiddens),\n",
    "                                    self.relu)\n",
    "        self.block2 = nn.Sequential(nn.Conv2d(in_channels=num_hiddens, out_channels=2*num_hiddens, kernel_size=(3,3), stride=(2,2), padding=1),\n",
    "                                    nn.BatchNorm2d(2*num_hiddens),\n",
    "                                    self.relu)\n",
    "\n",
    "        self.block3 = nn.Sequential(nn.Conv2d(in_channels=2*num_hiddens, out_channels=2*2*num_hiddens, kernel_size=(3,3), stride=(2,2), padding=1),\n",
    "                                    nn.BatchNorm2d(2*2*num_hiddens),\n",
    "                                    self.relu)\n",
    "        \n",
    "        self.block4 = nn.Sequential(nn.Conv2d(in_channels=2*2*num_hiddens, out_channels=2*2*2*num_hiddens, kernel_size=(3,3), stride=(4,2), padding=1),\n",
    "                                    nn.BatchNorm2d(2*2*2*num_hiddens),\n",
    "                                    self.relu)\n",
    "        self.rearrange = Rearrange('n c l f -> n l (c f)')\n",
    "        self.gru = nn.GRU(input_size = 2*2*2*num_hiddens*self.in_size[1]//16, hidden_size = 2*2*2*num_hiddens, batch_first = True)\n",
    "        self.fc0 = nn.Linear(2*2*2*num_hiddens, 8*latent)\n",
    "        self.fc1 = nn.Linear(8*latent, latent)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.block1(inputs)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.rearrange(x)\n",
    "        x, _ = self.gru(x)\n",
    "        x = self.relu(self.fc0(x[:,-1,:]))\n",
    "        return self.fc1(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fab770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity Check:\n",
    "#build a batch of size 2 and run it through the network\n",
    "test_net = CNN_RNN(1, 16, 11, (224,256))\n",
    "test_input = transforms.ToTensor()(mel_transform(train_feature_example_mono))\n",
    "test_input = torch.stack((test_input,test_input))\n",
    "\n",
    "x = test_net(test_input)\n",
    "print(test_input.shape, x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf51a5f0-dbb5-431b-9c4a-9556420d2058",
   "metadata": {},
   "source": [
    "For dual channel we dont use ToTensor() transform since mel already gives us the correct dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907ed26d-04e2-4ea7-bdfc-4217ef1a29e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity Check:\n",
    "#build a batch of size 3 for stereo and run it through the network\n",
    "test_net = CNN_RNN(2, 16, 11, (224,256))\n",
    "test_input = torch.tensor(mel_transform(train_feature_example))\n",
    "test_input = torch.stack((test_input,test_input,test_input))\n",
    "\n",
    "x = test_net(test_input)\n",
    "print(\"Input: {}, where batch size 3, dual channel audio 2, mel_spectogram: 224,256\\nOutput {}, where batch size: 3, labels: 11\".format(test_input.shape, x.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e654a4-3619-47ac-8db5-9db329501e09",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3a139e-0845-4880-a4c0-0e70cdd7a6e5",
   "metadata": {},
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5928b5e3-5fdf-4dc2-93a2-7264b6bfbb4f",
   "metadata": {},
   "source": [
    "Model architecture:<br>\n",
    "We use [CoAtNet](https://arxiv.org/pdf/2106.04803.pdf).<br>\n",
    "CoAtNet stands for \"Context-Aware Attention Network\", which is a type of machine learning model that uses attention mechanisms to better process input data. Attention mechanisms allow the model to focus on certain parts of the input data that are most relevant to the task at hand, rather than processing the entire input equally.<br>\n",
    "The CoAtNet model is typically composed of two main components: an encoder and a decoder. The encoder processes the input data and generates a \"context vector\" that represents the context of the input. The decoder then uses this context vector, along with the attention mechanism, to generate the final output.\n",
    "\n",
    "We use [this implementation](https://github.com/chinhsuanwu/coatnet-pytorch), with slight modifications to fit our needs.<br>\n",
    "It takes as input a 224x224 image and outputs a prediction.\n",
    "We adapted the data set dimentions to match the CoAtNet, i.e. we used random crop to get spectograms of the size 224x224.  \n",
    "\n",
    "We use coatnet_0 model which we have modified to get the following parameters:\n",
    "- image_size - the dimensions of the input image_size.\n",
    "- in_channels - the number of input channels.\n",
    "- num_classes - the number of data classes.\n",
    "\n",
    "Note: the original open-source model gets only num_classes parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d532474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coatnet import coatnet_0, count_parameters\n",
    "\n",
    "test_net = coatnet_0(image_size = (224,224), in_channels=1, num_classes=len(one_hot))\n",
    "test_input = torch.randn(2, 1, 224, 224)\n",
    "x = test_net(test_input)\n",
    "\n",
    "print(test_input.shape, x.shape, count_parameters(test_net))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d93eb8b-5404-4f27-b05b-5c98d70aed9d",
   "metadata": {},
   "source": [
    "# Testing Data\n",
    "## Benchmark\n",
    "First we divide the input signal to multiple windows each with the length of the training data (3 seconds), and we overlap them by 25% (hyperparameter), and we run the divided signal through the network to get a prediction on each segment, we define this transform in the next subsection.\n",
    "\n",
    "Next, we will define 3 testing criteria for instrument recognition for varying length inputs, each criterion is calculated after predicting the classes:\n",
    "\n",
    "- Majority vote:<br>\n",
    "This metric involves selecting the instrument with the highest number of votes, based on a predetermined threshold. <br>\n",
    "However, when applied to our specific task, it would ignore the presence of accompanying instruments. This is because musical signals typically consist of multiple instruments playing together, with sounds often overlapping in time. In most cases, the accompaniment instruments are weaker in volume compared to lead instruments such as the piano.\n",
    "In our context, instruments with more votes win depending on a threshold in our case, we choose the candidates with appearances in atleast a third of the predictions.\n",
    "- S1 - the average:<br>\n",
    "This metric calculates the average probability of each instrument in the audio clip by taking the average of the sigmoid outputs class-wise (instrument-wise), then applying a threshold to determine the presence of an instrument. The threshold should be selected carefully since higher threshold results in improved precision but decreased recall, while a lower threshold leads to increased recall but decreased precision. <br>\n",
    "This approach aims to determine the mean probability of each instrument's presence in the audio clip, but it may not necessarily detect all instruments.\n",
    "- S2 - the normalized sum:<br>\n",
    "This method first sums the sigmoid outputs for each class, over the entire audio clip. The values are then normalized by dividing them by the maximum value among all classes, resulting in a scale between 0 and 1. Finally, a threshold is applied to determine the presence of each instrument. <br>\n",
    "This approach is based on the idea that humans perceive the dominant instrument in a relative sense, such that the strongest instrument is always detected and the presence of other instruments is determined based on their relative strength compared to the most active instrument. The normalization step is used to ensure that the relative strength of each instrument is accurately captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30c2e5-c13e-44d0-8ef9-8ed5cce7f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all of these methods inputs are the outputs of neural networks with no activation function\n",
    "#they return a tensor with indexes of the predicted instruments\n",
    "import collections\n",
    "\n",
    "majority_threshold = 3\n",
    "def majority_vote(pred):\n",
    "    instruments = pred.max(1).indices\n",
    "    count = collections.Counter(instruments.tolist())\n",
    "    return torch.tensor([k for k,v in count.items() if v >= len(instruments)//majority_threshold], dtype=torch.long, device=pred.device)\n",
    "\n",
    "s1_threshold = 0.1\n",
    "def s1_vote(pred):\n",
    "    # 0.02 <= threshold <= 0.18\n",
    "    #assume pred is the output of the last layer without an activation function\n",
    "    s1 = pred.sigmoid().sum(0)\n",
    "    s1 = nnF.threshold(s1, s1_threshold, 0.0)\n",
    "    s1 = s1.nonzero(as_tuple=True)\n",
    "    return s1[0]\n",
    "    \n",
    "s2_threshold = 0.4\n",
    "def s2_vote(pred):\n",
    "    # 0.2 <= threshold <= 0.6\n",
    "    #assume pred is the output of the last layer without an activation function\n",
    "    s2 = pred.sigmoid()\n",
    "    s2 = s2.sum(0)\n",
    "    s2 = s2.div_(s2.max())\n",
    "    s2 = nnF.threshold(s2, s2_threshold, 0.0)\n",
    "    s2 = s2.nonzero(as_tuple=True)\n",
    "    return s2[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acde78a1-4a45-4011-9128-d24e4ba0e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_t = torch.tensor([[0.0, 0.0, 0.1, 0.0],\n",
    "                       [0.0, 0.4, 0.0, 0.0],\n",
    "                       [0.0, 0.0, 1.2, 0.0]])\n",
    "print(\"majority: {}, s1: {}, s2: {}\".format(majority_vote(test_t), s1_vote(test_t), s2_vote(test_t)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb96e50-9228-42fd-b814-acc7aaaabaa7",
   "metadata": {},
   "source": [
    "## Test Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e79456c-9a39-4bdb-9161-f0bd8bf61206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This transformation takes data input data of varying length, cuts it up into equally lengthed pieces and overlaps those pieces together\n",
    "#using this transform yields a batch like tensor for each input with unfixed size, so multiple inputs cant be batched together.\n",
    "\n",
    "class test_data_cutter_transform(object):\n",
    "    def __init__(self, overlap_percent):\n",
    "        self.overlap_per = overlap_percent\n",
    "        \n",
    "    def __call__(self, wave):\n",
    "        mono = 2 if wave.shape[0] == 2 else 1\n",
    "        leng = wave.shape[-1]\n",
    "        if leng < 132299:\n",
    "            npad = [(0, 0)] * wave.ndim\n",
    "            npad[-1] = (0, 132299-leng)\n",
    "            wave = np.pad(wave, npad, constant_values=0.0)\n",
    "            leng = 132299\n",
    "        parts = int(np.floor((leng+self.overlap_per*132299)/132299))\n",
    "        windows = np.empty((parts, mono, 132299))\n",
    "        window_starts = (np.arange(parts, dtype=np.intc)*(132299-self.overlap_per*132299)).astype(int)\n",
    "        for i in range(parts):\n",
    "            windows[i] = wave[...,window_starts[i]:window_starts[i]+132299]\n",
    "        return windows\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0978d5b-e3bd-4205-8275-ae09522478a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "#transform = transforms.Compose([mel_frequency_transform()]), \\\n",
    "\n",
    "test_data = IRMASDataset(wav_dir=testingSet_folder, split='test', mono=False, \\\n",
    "                         transform = transforms.Compose([test_data_cutter_transform(0.25), mel_frequency_transform()]),\\\n",
    "                         target_transform = one_hot_transform(testing_data=True))\n",
    "\n",
    "\n",
    "#data loader can only do 1 batch a time because each sample has a different length\n",
    "test_data_loader = DataLoader(test_data, batch_size=None, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e7e29-054d-48e5-b040-8a34460fe2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(test_data_loader)\n",
    "for _ in range(10):\n",
    "    x, y = next(test_iter)\n",
    "    print(x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7f5d3-6ab4-4163-ba2a-5cf983550984",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "Our vote functions take as input the prediction from the networks and output a tensor where each element is an integer that represents a predicted class (instrument), to use this output with our automatic loggers, we need to transform this multilabel output into a multiclass output, then we can easily calculate the Precision, Recall and F1 scores and log them for the testing data.\n",
    "\n",
    "And so we define the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae6f4b-ea3a-4f89-908f-924dfe9be01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
    "\n",
    "def multilabel_to_multiclass(p):\n",
    "    #transforms a multi-label input input multi-class output\n",
    "    #input: p - tensor(dtype=tensor.int) where each element specifies a label number (from {s1,s2,majority}_vote functions)\n",
    "    #output: pred - tensor of size 11 where pred[i] == i+1 if i in p\n",
    "    #we add 1 so we can ignore 0 (didn't predict)\n",
    "    pred = torch.zeros([len(one_hot)], dtype=p.dtype, device=p.device)\n",
    "    pred[p] = p+1\n",
    "    return pred\n",
    "# [0, 4, 11]\n",
    "# [1, 0, 0, 0, 5, 0, ..., 12]\n",
    "#example usage:\n",
    "#metric = MulticlassPrecision(num_classes=len(one_hot)+1, average='micro' )\n",
    "#x, y = batch\n",
    "#y_hat = model(x)\n",
    "#metric(multilabel_to_multiclass(s1_vote(y_hat)), multilabel_to_multiclass(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb697ce",
   "metadata": {},
   "source": [
    "## PyTorch Lightning Class\n",
    "\n",
    "We will use PyTorch Lightning module to ease the process of training, we will build a generic class that takes a model as input, and trains it with a Dataloader that we will provide.\n",
    "\n",
    "To implement the class, we first implement the `__init__` function that initializes it, we can do it like a normal `nn.Module` where we implement all the blocks as we saw in class, but we abstracted that using an input that is an `nn.module` so we can use the same class for all our models, and of course the forward function which simply passes the input through the model.\n",
    "\n",
    "To train and validate the model we implement the `training_step` and `validation_step` functions, which receive a batch and batch_index as input, it simply passes the batch through the model, then calculates the loss function and logs it, the training step returns the loss while validation doesn't return a value, and PyTorch lightning takes care of the boilerplate code.\n",
    "\n",
    "We also need to add `configure_optimizers` which simply returns an adam optimizer on the parameters of the model.\n",
    "\n",
    "Thus we can run training on any model using the PyTorch lightning trainer class.\n",
    "\n",
    "For testing we saw earlier that each sample has a varying length, so the testing step needs to consider the input windows, we did so in the {majority,s1,s2}_vote functions that accept the output of the model on training data as inputs, and outputs the prediction, we then apply a transformation from a multilabel prediction to a multiclass prediction where multiple classes can be present, and calculate metrics such as the F1, Precision and Recall on them\n",
    "\n",
    "The rest of the functions implemented in the class were used at the start of the project to find out the maximum batch size for a model, but were abandoned after.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc871c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LModel(pl.LightningModule):\n",
    "    def __init__(self, model, data_loader=None, valid_loader=None, lr=0.001, batch_size=64):\n",
    "        super(LModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.data_loader = data_loader\n",
    "        \n",
    "        ####################\n",
    "        \n",
    "        self.precision_majority_micro = MulticlassPrecision(num_classes=len(one_hot)+1, multidim_average='global', average='micro', ignore_index=0, validate_args=False)\n",
    "        self.precision_majority_macro = MulticlassPrecision(num_classes=len(one_hot)+1, multidim_average='global', average='macro', ignore_index=0, validate_args=False)\n",
    "        \n",
    "        self.recall_majority_micro = MulticlassRecall(num_classes=len(one_hot)+1, multidim_average='global', average='micro', ignore_index=0, validate_args=False)\n",
    "        self.recall_majority_macro = MulticlassRecall(num_classes=len(one_hot)+1, multidim_average='global', average='macro', ignore_index=0, validate_args=False)\n",
    "        \n",
    "        self.f1score_majority_micro = MulticlassF1Score(num_classes=len(one_hot)+1, multidim_average='global', average='micro', ignore_index=0, validate_args=False)\n",
    "        self.f1score_majority_macro = MulticlassF1Score(num_classes=len(one_hot)+1, multidim_average='global', average='macro', ignore_index=0, validate_args=False)\n",
    "        ####################\n",
    "        \n",
    "        self.precision_s1_micro = MulticlassPrecision(num_classes=len(one_hot)+1, multidim_average='global', average='micro', ignore_index=0, validate_args=False)\n",
    "        self.precision_s1_macro = MulticlassPrecision(num_classes=len(one_hot)+1, multidim_average='global', average='macro', ignore_index=0, validate_args=False)\n",
    "        \n",
    "        self.recall_s1_micro = MulticlassRecall(num_classes=len(one_hot)+1, multidim_average='global', average='micro', ignore_index=0, validate_args=False)\n",
    "        self.recall_s1_macro = MulticlassRecall(num_classes=len(one_hot)+1, multidim_average='global', average='macro', ignore_index=0, validate_args=False)\n",
    "        \n",
    "        self.f1score_s1_micro = MulticlassF1Score(num_classes=len(one_hot)+1, multidim_average='global', average='micro', ignore_index=0, validate_args=False)\n",
    "        self.f1score_s1_macro = MulticlassF1Score(num_classes=len(one_hot)+1, multidim_average='global', average='macro', ignore_index=0, validate_args=False)\n",
    "        ####################\n",
    "        \n",
    "        self.precision_s2_micro = MulticlassPrecision(num_classes=len(one_hot)+1, multidim_average='global', average='micro', ignore_index=0, validate_args=False)\n",
    "        self.precision_s2_macro = MulticlassPrecision(num_classes=len(one_hot)+1, multidim_average='global', average='macro', ignore_index=0, validate_args=False)\n",
    "        \n",
    "        self.recall_s2_micro = MulticlassRecall(num_classes=len(one_hot)+1, multidim_average='global', average='micro', ignore_index=0, validate_args=False)\n",
    "        self.recall_s2_macro = MulticlassRecall(num_classes=len(one_hot)+1, multidim_average='global', average='macro', ignore_index=0, validate_args=False)\n",
    "        \n",
    "        self.f1score_s2_micro = MulticlassF1Score(num_classes=len(one_hot)+1, multidim_average='global', average='micro', ignore_index=0, validate_args=False)\n",
    "        self.f1score_s2_macro = MulticlassF1Score(num_classes=len(one_hot)+1, multidim_average='global', average='macro', ignore_index=0, validate_args=False)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return self.model(inputs)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = nnF.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = nnF.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        majority = multilabel_to_multiclass(majority_vote(y_hat))\n",
    "        s1 = multilabel_to_multiclass(s1_vote(y_hat))\n",
    "        s2 = multilabel_to_multiclass(s2_vote(y_hat))\n",
    "        y_multiclass = multilabel_to_multiclass(y)\n",
    "        \n",
    "        ####################\n",
    "        \n",
    "        self.precision_majority_micro(majority, y_multiclass)\n",
    "        self.log(\"P_majority_micro\", self.precision_majority_micro, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.precision_majority_macro(majority, y_multiclass)\n",
    "        self.log(\"P_majority_macro\", self.precision_majority_macro, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        \n",
    "        self.recall_majority_micro(majority, y_multiclass)\n",
    "        self.log(\"R_majority_micro\", self.recall_majority_micro, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.recall_majority_macro(majority, y_multiclass)\n",
    "        self.log(\"R_majority_macro\", self.recall_majority_macro, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        \n",
    "        self.f1score_majority_micro(majority, y_multiclass)\n",
    "        self.log(\"F1_majority_micro\", self.f1score_majority_micro, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.f1score_majority_macro(majority, y_multiclass)\n",
    "        self.log(\"F1_majority_macro\", self.f1score_majority_macro, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        ####################\n",
    "        \n",
    "        self.precision_s1_micro(s1, y_multiclass)\n",
    "        self.log(\"P_s1_micro\", self.precision_s1_micro, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.precision_s1_macro(s1, y_multiclass)\n",
    "        self.log(\"P_s1_macro\", self.precision_s1_macro, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        \n",
    "        self.recall_s1_micro(s1, y_multiclass)\n",
    "        self.log(\"R_s1_micro\", self.recall_s1_micro, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.recall_s1_macro(s1, y_multiclass)\n",
    "        self.log(\"R_s1_macro\", self.recall_s1_macro, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        \n",
    "        self.f1score_s1_micro(s1, y_multiclass)\n",
    "        self.log(\"F1_s1_micro\", self.f1score_s1_micro, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.f1score_s1_macro(s1, y_multiclass)\n",
    "        self.log(\"F1_s1_macro\", self.f1score_s1_macro, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        ####################\n",
    "        \n",
    "        self.precision_s2_micro(s2, y_multiclass)\n",
    "        self.log(\"P_s2_micro\", self.precision_s2_micro, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.precision_s2_macro(s2, y_multiclass)\n",
    "        self.log(\"P_s2_macro\", self.precision_s2_macro, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        \n",
    "        self.recall_s2_micro(s2, y_multiclass)\n",
    "        self.log(\"R_s2_micro\", self.recall_s2_micro, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.recall_s2_macro(s2, y_multiclass)\n",
    "        self.log(\"R_s2_macro\", self.recall_s2_macro, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        \n",
    "        self.f1score_s2_micro(s2, y_multiclass)\n",
    "        self.log(\"F1_s2_micro\", self.f1score_s2_micro, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.f1score_s2_macro(s2, y_multiclass)\n",
    "        self.log(\"F1_s2_macro\", self.f1score_s2_macro, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr = self.lr)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "         return self.data_loader\n",
    "        \n",
    "    def valid_dataloader(self):\n",
    "         return self.data_loader\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae429fc-7276-494a-88c4-c7661b23f878",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "First, we define a function that fetches the most recently trained model, to allow stopping and resuming the training process, And another that fetches the best model based on validation loss, to use in testing the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a1acfe-cea6-4ad8-9df1-9b032ecf283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function gets the latest checkpoint for a model to resume training once we stopped\n",
    "#if it doesn't find a checkpoints it returns None so the trainer will start anew.\n",
    "def get_latest_checkpoint(dire):\n",
    "    try:\n",
    "        path = os.path.join(glob.glob(os.path.join(dire, \"lightning_logs\", \"version_*\"))[-1], \"checkpoints\", \"last.ckpt\")\n",
    "    except:\n",
    "        return None\n",
    "    return path if os.path.exists(path) else None\n",
    "\n",
    "#this function hopefully gets the best model based on validation loss, will throw an error if a model wasn't found.\n",
    "#it is meant to use in the testing section\n",
    "def get_latest_checkpoint2(dire):\n",
    "    path = os.path.join(glob.glob(os.path.join(dire, \"lightning_logs\", \"version_*\"))[-1], \"checkpoints\", \"*\")\n",
    "    files = glob.glob(path)\n",
    "    return (files[1]) if files else None\n",
    "    \n",
    "#to save the last training and resume later\n",
    "mc = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", save_last=True, save_top_k=2, mode=\"min\") #always reinitialize beofre using\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffbb9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_net = LModel(coatnet_0(image_size = (224,224), in_channels=1, num_classes=len(one_hot)))\n",
    "# test_input = torch.randn(2, 1, 224, 224)\n",
    "# x = test_net(test_input)\n",
    "# print(x.shape, count_parameters(test_net))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9fcd62-20e4-4507-b681-6d167bafd002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we setup the training datasets with caching, it spawns multiple processes but speed up the training by a large margin (over x40 times)\n",
    "#preferably call dataset.clear() after being done with them to clear the cache and free up the memory.\n",
    "train_data_mono = IRMASDataset(wav_dir=trainingSet_folder, split='train', cache=True, mono=True, \\\n",
    "                          transform = transforms.Compose([mel_frequency_transform(), transforms.ToTensor()]), \\\n",
    "                          target_transform = one_hot)\n",
    "\n",
    "valid_data_mono = IRMASDataset(wav_dir=trainingSet_folder, split='valid', cache=True, mono=True, \\\n",
    "                          transform = transforms.Compose([mel_frequency_transform(), transforms.ToTensor()]), \\\n",
    "                          target_transform = one_hot)\n",
    "\n",
    "train_data_stereo = IRMASDataset(wav_dir=trainingSet_folder, split='train', cache=True, \\\n",
    "                          transform = transforms.Compose([mel_frequency_transform()]), \\\n",
    "                          target_transform = one_hot)\n",
    "\n",
    "valid_data_stereo = IRMASDataset(wav_dir=trainingSet_folder, split='valid', cache=True, \\\n",
    "                          transform = transforms.Compose([mel_frequency_transform()]), \\\n",
    "                          target_transform = one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0f9e73-3b99-4487-800a-237437904d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary with model names and paths and models, used for testing the data later\n",
    "models_info = {} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40735eb6-ba64-465b-95fb-332a7134a4c1",
   "metadata": {},
   "source": [
    "Next, we will define the models themselves and run the training on them, we stopped training when the validation loss didn't improve for at least 15 epochs, or the model ran for 100 epochs.\n",
    "\n",
    "Also after we define the model, we insert its name and directory and the lighting torch module of the model into the `models_info` dictionary, so we can run the testing loops on them later.\n",
    "\n",
    "If the models were already trained and we just want to run the testing on them we only run the cells where we define the models (and insert them into the dictionary), and simply go down and run the testing cells, this gave us incredible flexibility to run a subset of the models if the notebook/pc crashed which it did, many times :)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2e0628-7b4f-4148-9b1f-39b3add46b99",
   "metadata": {},
   "source": [
    "#### Model 1 - mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952a97a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del test_net\n",
    "gc.collect()\n",
    "num_hiddens = 32\n",
    "latent = len(one_hot)\n",
    "in_size = (224,256)\n",
    "model_1_mono_dir = os.path.join(os.getcwd(), \"checkpoints\", \"model_1_mono\")\n",
    "model_to_train_1 = SIMPLE_CNN(1, num_hiddens, latent, in_size)\n",
    "\n",
    "train_data_loader_1 = DataLoader(train_data_mono, batch_size=256, shuffle=True, num_workers=2, prefetch_factor=6)\n",
    "\n",
    "valid_data_loader_1 = DataLoader(valid_data_mono, batch_size=128, num_workers=1, prefetch_factor=6)\n",
    "\n",
    "model_1_mono = LModel(model_to_train_1)#, train_data_loader_1, valid_data_loader_1) #test here and in fit\n",
    "\n",
    "models_info[\"model_1_mono\"] = [model_1_mono, model_1_mono_dir]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0e392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mc = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", save_last=True, save_top_k=2, mode=\"min\") #always put this line before trainer so it saves to the correct place\n",
    "trainer = pl.Trainer(accelerator=\"auto\", callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=15),mc], \\\n",
    "                     max_epochs=200, default_root_dir=model_1_mono_dir, log_every_n_steps=10)#, auto_scale_batch_size=\"binsearch\")\n",
    "#trainer.tune(model_1)\n",
    "trainer.fit(model_1_mono, train_dataloaders=train_data_loader_1, val_dataloaders=valid_data_loader_1, ckpt_path=get_latest_checkpoint(model_1_mono_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08c1cc9-ffa4-42c7-aa36-353c6b02b76f",
   "metadata": {},
   "source": [
    "#### Model 1 - stereo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e774490-9761-4809-a7b8-02fecce5abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del test_net\n",
    "gc.collect()\n",
    "num_hiddens = 32\n",
    "latent = len(one_hot)\n",
    "in_size = (224,256)\n",
    "model_1_stereo_dir = os.path.join(os.getcwd(), \"checkpoints\", \"model_1_stereo\")\n",
    "model_to_train_1_stereo = SIMPLE_CNN(2, num_hiddens, latent, in_size)\n",
    "\n",
    "train_data_loader_1_stereo = DataLoader(train_data_stereo, batch_size=256, shuffle=True, num_workers=2, prefetch_factor=6)\n",
    "\n",
    "valid_data_loader_1_stereo = DataLoader(valid_data_stereo, batch_size=128, num_workers=1, prefetch_factor=6)\n",
    "\n",
    "model_1_stereo = LModel(model_to_train_1_stereo)#, train_data_loader_1, valid_data_loader_1) #test here and in fit\n",
    "\n",
    "models_info[\"model_1_stereo\"] = [model_1_stereo, model_1_stereo_dir]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a771b83-041a-40aa-853e-6103bab9613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mc = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", save_last=True, save_top_k=2, mode=\"min\") #always put this line before trainer so it saves to the correct place\n",
    "trainer = pl.Trainer(accelerator=\"auto\", callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=15),mc], \\\n",
    "                     max_epochs=200, log_every_n_steps=10, default_root_dir=model_1_stereo_dir)#, auto_scale_batch_size=\"binsearch\")\n",
    "#trainer.tune(model_1)\n",
    "trainer.fit(model_1_stereo, train_dataloaders=train_data_loader_1_stereo, val_dataloaders=valid_data_loader_1_stereo, ckpt_path=get_latest_checkpoint(model_1_stereo_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72472851-f0b7-4278-90d8-9812d225abdd",
   "metadata": {},
   "source": [
    "#### Model 2 - mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4b66fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "num_hiddens = 32\n",
    "latent = len(one_hot)\n",
    "in_size = (224,256)\n",
    "model_2_mono_dir = os.path.join(os.getcwd(), \"checkpoints\", \"model_2_mono\")\n",
    "model_to_train_2 = CNN_RNN(1, num_hiddens, latent, in_size)\n",
    "\n",
    "train_data_loader_2 = DataLoader(train_data_mono, batch_size=256, shuffle=True, num_workers=2, prefetch_factor=8)\n",
    "\n",
    "valid_data_loader_2 = DataLoader(valid_data_mono, batch_size=128, num_workers=1, prefetch_factor=2)\n",
    "\n",
    "model_2_mono = LModel(model_to_train_2)#, train_data_loader_2, valid_data_loader_2)\n",
    "\n",
    "models_info[\"model_2_mono\"] = [model_2_mono, model_2_mono_dir]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mc = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", save_last=True, save_top_k=2, mode=\"min\") #always put this line before trainer so it saves to the correct place\n",
    "trainer = pl.Trainer(accelerator=\"auto\", callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=15),mc], \\\n",
    "                     max_epochs=200, log_every_n_steps=10, default_root_dir=model_2_mono_dir)#, auto_scale_batch_size=\"binsearch\")\n",
    "#trainer.tune(model_1)\n",
    "trainer.fit(model_2_mono, train_dataloaders=train_data_loader_2, val_dataloaders=valid_data_loader_2, ckpt_path=get_latest_checkpoint(model_2_mono_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e77f4-f6ab-41c2-8d5b-88d616fab50f",
   "metadata": {},
   "source": [
    "#### Model 2 - mono - leaky ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c9fd01-0bb0-4e39-8b5a-7ca2018b6026",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "num_hiddens = 32\n",
    "latent = len(one_hot)\n",
    "in_size = (224,256)\n",
    "model_2_mono_lrelu_dir = os.path.join(os.getcwd(), \"checkpoints\", \"model_2_mono_lrelu\")\n",
    "model_to_train_2 = CNN_RNN(1, num_hiddens, latent, in_size, relu_type=\"leaky_relu\", neg_slope=0.3)\n",
    "\n",
    "train_data_loader_2 = DataLoader(train_data_mono, batch_size=256, shuffle=True, num_workers=2, prefetch_factor=8)\n",
    "\n",
    "valid_data_loader_2 = DataLoader(valid_data_mono, batch_size=128, num_workers=1, prefetch_factor=2)\n",
    "\n",
    "model_2_mono_lrelu = LModel(model_to_train_2)#, train_data_loader_2, valid_data_loader_2)\n",
    "\n",
    "models_info[\"model_2_mono_lrelu\"] = [model_2_mono_lrelu, model_2_mono_lrelu_dir]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f584653-a067-46b5-898d-048f33a70e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mc = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", save_last=True, save_top_k=2, mode=\"min\") #always put this line before trainer so it saves to the correct place\n",
    "trainer = pl.Trainer(accelerator=\"auto\", callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=15),mc], \\\n",
    "                     max_epochs=200, log_every_n_steps=10, default_root_dir=model_2_mono_lrelu_dir)#, auto_scale_batch_size=\"binsearch\")\n",
    "#trainer.tune(model_1)\n",
    "trainer.fit(model_2_mono_lrelu, train_dataloaders=train_data_loader_2, val_dataloaders=valid_data_loader_2, ckpt_path=get_latest_checkpoint(model_2_mono_lrelu_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5d3e66-7a50-4c90-b910-ae72c799181d",
   "metadata": {},
   "source": [
    "#### Model 2 - stereo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef77d8-7035-4e6d-be78-fec02c6f2c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "num_hiddens = 32\n",
    "latent = len(one_hot)\n",
    "in_size = (224,256)\n",
    "model_2_stereo_dir = os.path.join(os.getcwd(), \"checkpoints\", \"model_2_stereo\")\n",
    "model_to_train_2 = CNN_RNN(2, num_hiddens, latent, in_size)\n",
    "\n",
    "train_data_loader_2 = DataLoader(train_data_stereo, batch_size=256, shuffle=True, num_workers=2, prefetch_factor=8)\n",
    "\n",
    "valid_data_loader_2 = DataLoader(valid_data_stereo, batch_size=128, num_workers=1, prefetch_factor=2)\n",
    "\n",
    "model_2_stereo = LModel(model_to_train_2)#, train_data_loader_2, valid_data_loader_2)\n",
    "\n",
    "models_info[\"model_2_stereo\"] = [model_2_stereo, model_2_stereo_dir]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b9bbc4-2314-41b0-87cb-f542691f8be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mc = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", save_last=True, save_top_k=2, mode=\"min\") #always put this line before trainer so it saves to the correct place\n",
    "trainer = pl.Trainer(accelerator=\"auto\", callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=15),mc], \\\n",
    "                     max_epochs=200, log_every_n_steps=10, default_root_dir=model_2_stereo_dir)#, auto_scale_batch_size=\"binsearch\")\n",
    "#trainer.tune(model_1)\n",
    "trainer.fit(model_2_stereo, train_dataloaders=train_data_loader_2, val_dataloaders=valid_data_loader_2, ckpt_path=get_latest_checkpoint(model_2_stereo_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74f7e0-c9a5-4e43-adc7-b2dc057d1590",
   "metadata": {},
   "source": [
    "#### Model 2 - stereo - leaky ReLu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f74ea48-5106-4a9a-ba73-d4602b9761fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "num_hiddens = 32\n",
    "latent = len(one_hot)\n",
    "in_size = (224,256)\n",
    "model_2_stereo_lrelu_dir = os.path.join(os.getcwd(), \"checkpoints\", \"model_2_stereo_lrelu\")\n",
    "model_to_train_2 = CNN_RNN(2, num_hiddens, latent, in_size, relu_type=\"leaky_relu\", neg_slope=0.3)\n",
    "\n",
    "train_data_loader_2 = DataLoader(train_data_stereo, batch_size=256, shuffle=True, num_workers=2, prefetch_factor=8)\n",
    "\n",
    "valid_data_loader_2 = DataLoader(valid_data_stereo, batch_size=128, num_workers=1, prefetch_factor=2)\n",
    "\n",
    "model_2_stereo_lrelu = LModel(model_to_train_2)#, train_data_loader_2, valid_data_loader_2)\n",
    "\n",
    "models_info[\"model_2_stereo_lrelu\"] = [model_2_stereo_lrelu, model_2_stereo_lrelu_dir]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474dae54-074e-4f9a-a69c-e453b1d299af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mc = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", save_last=True, save_top_k=2, mode=\"min\") #always put this line before trainer so it saves to the correct place\n",
    "trainer = pl.Trainer(accelerator=\"auto\", callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=15),mc], \\\n",
    "                     max_epochs=200, log_every_n_steps=10, default_root_dir=model_2_stereo_lrelu_dir)#, auto_scale_batch_size=\"binsearch\")\n",
    "#trainer.tune(model_1)\n",
    "trainer.fit(model_2_stereo_lrelu, train_dataloaders=train_data_loader_2, val_dataloaders=valid_data_loader_2, ckpt_path=get_latest_checkpoint(model_2_stereo_lrelu_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82561a12-0240-451a-9f73-c1249d8d460b",
   "metadata": {},
   "source": [
    "#### Model 3 - mono:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368de543",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "num_hiddens = 32\n",
    "latent = len(one_hot)\n",
    "in_size = (224,224)\n",
    "model_3_mono_dir = os.path.join(os.getcwd(), \"checkpoints\", \"model_3_mono\")\n",
    "model_to_train_3 = coatnet_0(image_size = in_size, in_channels=1, num_classes=latent)\n",
    "\n",
    "train_data_3 = IRMASDataset(wav_dir=trainingSet_folder, split='train', mono=True, cache = True, \\\n",
    "                          transform = transforms.Compose([mel_frequency_transform(), transforms.ToTensor(), transforms.CenterCrop(224)]), \\\n",
    "                          target_transform = one_hot) #add randomCrop\n",
    "\n",
    "valid_data_3 = IRMASDataset(wav_dir=trainingSet_folder, split='valid', mono=True, cache = True, \\\n",
    "                          transform = transforms.Compose([mel_frequency_transform(), transforms.ToTensor(), transforms.CenterCrop(224)]), \\\n",
    "                          target_transform = one_hot) #add randomCrop\n",
    "\n",
    "train_data_loader_3 = DataLoader(train_data_3, batch_size=32, shuffle=True, num_workers=3, prefetch_factor=4)\n",
    "\n",
    "valid_data_loader_3 = DataLoader(valid_data_3, batch_size=8, num_workers=1, prefetch_factor=2)\n",
    "\n",
    "model_3_mono = LModel(model_to_train_3)#, train_data_loader_3, valid_data_loader_3)\n",
    "\n",
    "models_info[\"model_3_mono\"] = [model_3_mono, model_3_mono_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6afefdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mc = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", save_last=True, save_top_k=2, mode=\"min\") #always put this line before trainer so it saves to the correct place\n",
    "trainer = pl.Trainer(accelerator=\"auto\", callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10),mc], \\\n",
    "                     max_epochs=100, default_root_dir=model_3_mono_dir)#, auto_scale_batch_size=\"binsearch\")\n",
    "#trainer.tune(model_1)\n",
    "trainer.fit(model_3_mono, train_dataloaders=train_data_loader_3, val_dataloaders=valid_data_loader_3, ckpt_path=get_latest_checkpoint(model_3_mono_dir))\n",
    "\n",
    "train_data_3.clear()\n",
    "valid_data_3.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16d33d0-97cb-43bf-bc4b-6b715eeac955",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a3112-6dfc-4cac-ad3a-656073ebc4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we set caching to true\n",
    "#preferably clear the training dataloaders before starting to test as to avoid crashing\n",
    "one_hot_test_data = one_hot_transform(testing_data=True)\n",
    "#transform = transforms.Compose([mel_frequency_transform()]), \\\n",
    "test_data_mono = IRMASDataset(wav_dir=testingSet_folder, split='test', mono=True, cache=True, \\\n",
    "                         transform = transforms.Compose([test_data_cutter_transform(0.25), mel_frequency_transform(), lambda x: torch.tensor(x, dtype=torch.float)]),\\\n",
    "                         target_transform = one_hot_test_data)\n",
    "#data loader can only do 1 batch a time because each sample has a different length\n",
    "test_data_mono_loader = DataLoader(test_data_mono, batch_size=None, shuffle=False)#, num_workers=2, prefetch_factor=4)\n",
    "\n",
    "test_data_stereo = IRMASDataset(wav_dir=testingSet_folder, split='test', cache=True, \\\n",
    "                          transform = transforms.Compose([test_data_cutter_transform(0.25), mel_frequency_transform(), lambda x: torch.tensor(x, dtype=torch.float)]), \\\n",
    "                          target_transform = one_hot_test_data)\n",
    "\n",
    "test_data_stereo_loader = DataLoader(test_data_stereo, batch_size=None, shuffle=False)#, num_workers=2, prefetch_factor=4)\n",
    "\n",
    "test_data_3_mono = IRMASDataset(wav_dir=testingSet_folder, split='test', mono=True, cache=False, \\\n",
    "                          transform = transforms.Compose([test_data_cutter_transform(0.25), mel_frequency_transform(), lambda x: torch.tensor(x, dtype=torch.float), transforms.CenterCrop(224)]), \\\n",
    "                          target_transform = one_hot_test_data) #add randomCrop\n",
    "\n",
    "test_data_3_mono_loader = DataLoader(test_data_3_mono, batch_size=None, shuffle=False)#, num_workers=2, prefetch_factor=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b24819-0f8a-44e1-9022-8f3a9b89dee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data_mono[0][0].dtype)\n",
    "test_data_stereo[0][0].dtype\n",
    "test_data_3_mono[1][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e407453-4e97-4b4d-854c-a876673c45a9",
   "metadata": {},
   "source": [
    "The next cell is used to define the csv file that we save our testing results into, and a function that adds a row to the said csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22ddf5c-06f8-413a-86f9-638ca191df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "key_list = ['F1_majority_macro', 'F1_s1_macro', 'F1_s2_macro',\\\n",
    "            'F1_majority_micro', 'F1_s1_micro', 'F1_s2_micro',\\\n",
    "            'P_majority_macro', 'P_s1_macro', 'P_s2_macro',\\\n",
    "            'P_majority_micro', 'P_s1_micro', 'P_s2_micro',\\\n",
    "            'R_majority_macro', 'R_s1_macro', 'R_s2_macro',\\\n",
    "            'R_majority_micro', 'R_s1_micro', 'R_s2_micro']\n",
    "\n",
    "if not os.path.isfile(\"results.csv\"):\n",
    "    with open(\"results.csv\", 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        row = ['model_name', 'majority_threshold', 's1_threshold', 's2_threshold']\n",
    "        row.extend(key_list)\n",
    "        writer.writerow(row)\n",
    "    \n",
    "def csv_writer(model_name, dic):\n",
    "    with open(\"results.csv\", 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        row = [model_name, majority_threshold, s1_threshold, s2_threshold]\n",
    "        row.extend([dic[key] for key in key_list])\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6290003-b24e-4439-85f8-5205417b20c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear the training datasets so we can cache the testing datasets\n",
    "train_data_mono.clear()\n",
    "valid_data_mono.clear()\n",
    "train_data_stereo.clear()\n",
    "valid_data_stereo.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef6c76-2213-4c18-abfe-eaf96a9ab47d",
   "metadata": {},
   "source": [
    "Here we will pop elements of the dictionary `models_info` by first popping all \"mono\" models and then \"stereo\" and then \"model_3\", we did it this way to save on precious ram and utilize caching of only one testing set at a time as to avoid crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449af230-1338-407f-97e9-974afa9c72d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_keys = models_info.keys()\n",
    "model_keys = sorted(model_keys, key = lambda x: x.split('_')[2])\n",
    "mono_keys = [key for key in model_keys if \"mono\" in key and \"3\" not in key]\n",
    "stereo_keys = [key for key in model_keys if \"stereo\" in key]\n",
    "\n",
    "for name in mono_keys:\n",
    "    direc = models_info.pop(name)\n",
    "    trainer = pl.Trainer(accelerator=\"auto\", enable_checkpointing=False)#, \\\n",
    "                     #limit_test_batches=0.1,)#, auto_scale_batch_size=\"binsearch\")\n",
    "    data_loader = test_data_stereo_loader if \"stereo\" in name else test_data_3_mono_loader if \"model_3\" in name else test_data_mono_loader\n",
    "    metrics_model = trainer.test(direc[0], verbose=False, dataloaders=data_loader, ckpt_path=get_latest_checkpoint2(direc[1]))\n",
    "    csv_writer(name, metrics_model[0])\n",
    "    del direc[:]\n",
    "test_data_mono.clear()\n",
    "del test_data_mono_loader\n",
    "gc.collect()\n",
    "\n",
    "for name in stereo_keys:\n",
    "    direc = models_info.pop(name)\n",
    "    trainer = pl.Trainer(accelerator=\"auto\", enable_checkpointing=False)#, \\\n",
    "                     #limit_test_batches=0.1,)#, auto_scale_batch_size=\"binsearch\")\n",
    "    data_loader = test_data_stereo_loader if \"stereo\" in name else test_data_3_mono_loader if \"model_3\" in name else test_data_mono_loader\n",
    "    metrics_model = trainer.test(direc[0], verbose=False, dataloaders=data_loader, ckpt_path=get_latest_checkpoint2(direc[1]))\n",
    "    csv_writer(name, metrics_model[0])\n",
    "    del direc[:]\n",
    "test_data_stereo.clear()\n",
    "del test_data_stereo_loader\n",
    "gc.collect()\n",
    "\n",
    "while models_info:\n",
    "    name, direc = models_info.popitem()\n",
    "    trainer = pl.Trainer(accelerator=\"auto\", enable_checkpointing=False)#, \\\n",
    "                 #limit_test_batches=0.1,)#, auto_scale_batch_size=\"binsearch\")\n",
    "    #can probably do the next line better\n",
    "    data_loader = test_data_stereo_loader if \"stereo\" in name else test_data_3_mono_loader if \"model_3\" in name else test_data_mono_loader\n",
    "    metrics_model = trainer.test(direc[0], verbose=False, dataloaders=data_loader, ckpt_path=get_latest_checkpoint2(direc[1]))\n",
    "    csv_writer(name, metrics_model[0])\n",
    "    del direc[:]\n",
    "\n",
    "test_data_mono.clear()\n",
    "test_data_stereo.clear()\n",
    "test_data_3_mono.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc48cff6-e9d7-4b92-9fed-4c129ff6c26e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Results\n",
    "To evaluate the performance of our proposed models, we used 3 methods: precision, recall and F1 measure. <br>\n",
    "**Precision - P:** <br>\n",
    "$$P = \\frac{t_p}{t_p+f_p}$$ \n",
    "where $t_p$ is true positive and $f_p$ is false positive.\n",
    "This measurement is used to measure the proportion of positive identifications that is actually correct. <br>\n",
    "**Recall - R:** <br> \n",
    "$$R = \\frac{t_p}{t_p+f_n}$$\n",
    "where $t_p$ is true positive and $f_n$ is false negative.\n",
    "This measurement is used to measure the proportion of actual positives that was identified correctly.<br>\n",
    "**F1 measure:** <br>\n",
    "$$F1 = \\frac{2PR}{P+R}$$\n",
    "Where P and R are the precision and recall respectively.\n",
    "\n",
    "Precision and recall offer a trade-off, i.e., one metric comes at the cost of another. More precision involves a harsher critic (classifier) that doubts even the actual positive samples from the dataset, thus reducing the recall score. On the other hand, more recall entails a lax critic that allows any sample that resembles a positive class to pass, which makes border-case negative samples classified as positive, thus reducing the precision. Ideally, we want to maximize both precision and recall metrics to obtain the perfect classifier.\n",
    "\n",
    "We used the F1 measure since it is more reliable because it calculates the overall performance of the models.\n",
    "\n",
    "\n",
    "We also calculated micro and macro averages:<br>\n",
    "**Macro:** \n",
    "    The macro average precision is the arithmetic mean of all the precision values for the different classes.<br> \n",
    "**Micro:**\n",
    "    The micro average precision is the sum of all true positives divided by the sum of all true positives and false positives.<br>\n",
    "Where the difference between micro and macro averaging is that macro averaging gives equal weight to each category while micro averaging gives equal weight to each sample.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8deccd1-d15f-4ee2-8eab-6f34a1b64f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the csv and sorting the results and filtering for the relevant information.\n",
    "results_16hidden = pd.read_csv('results_16_bce.csv', index_col=0)\n",
    "results_16hidden = results_16hidden.sort_index().filter(regex=\"_majority|_s1|_s2\", axis=1)\n",
    "results_16hidden = results_16hidden.reindex(sorted(results_16hidden.columns,\\\n",
    "                                            key=lambda x: (x.split('_')[0], x.split('_')[2], x.split('_')[1])), axis=1)\n",
    "\n",
    "display(results_16hidden.T.tail())\n",
    "\n",
    "results_32hidden = pd.read_csv('results_32_bce.csv', index_col=0)\n",
    "results_32hidden = results_32hidden.sort_index().filter(regex=\"_majority|_s1|_s2\", axis=1)\n",
    "results_32hidden = results_32hidden.reindex(sorted(results_32hidden.columns,\\\n",
    "                                            key=lambda x: (x.split('_')[0], x.split('_')[2], x.split('_')[1])), axis=1)\n",
    "display(results_32hidden.T.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f55d6d9-99b5-40cb-a8fc-a1832f5be08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a modular helper function that makes plotting easier\n",
    "\n",
    "def bar_plotter(results, kind, model_kind=None, show=True, ax=None, hidden=16, mt='1/3', s1t='0.1', s2t='0.4', legend=True, ylim=1):\n",
    "    #plots the data in a bar graph\n",
    "    #results is the pandas dataframe that is sorted as we wanted\n",
    "    #kind is a string literal in [\"F1\", \"R\", \"P\"]\n",
    "    #model_kind is a regular expression string that is used to filter the model names to compare different models\n",
    "    #mt -> majority threshold used, can be found in results csv\n",
    "    #s1t -> s1 threshold used, can be found in results csv\n",
    "    #s2t -> s2 threshold used, can be found in results csv\n",
    "    df = results.filter(like=kind+\"_\").T\n",
    "    if model_kind:\n",
    "        df = df.filter(regex=model_kind)\n",
    "    ax = df.plot(\n",
    "            kind='bar',\n",
    "            ax=ax,\n",
    "            stacked=False,\n",
    "            title=r'{} Score with $\\bf{{{}}}$ hidden layers{}Majority Threshold: {}, S1 Threshold: {}, S2 Threshold: {}'.format(kind, hidden, '\\n', mt, s1t, s2t),\n",
    "            figsize=(12,6),\n",
    "            zorder=3,\n",
    "            alpha=0.9)\n",
    "    ax.set_ylim(0,ylim)\n",
    "    ax.legend(prop={'size': 8})\n",
    "    ax.grid(visible=True, color='k', linestyle='--', alpha=0.8, zorder=1, which='both') #ax.grid(axis='y', linestyle='--')\n",
    "    ax.set_xticklabels(['Majority','S1\\nMacro','S2','Majority','S1\\nMicro','S2'], rotation='horizontal')\n",
    "    if not legend:\n",
    "        ax.get_legend().remove()\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced9ea2-caff-4c71-b48f-56efa24da811",
   "metadata": {},
   "source": [
    "Here we will plot the results of all the models:\n",
    "\n",
    "We can see that model 3 beat the rest by a large margin in all metrics, we can also see that all the precision (P) scores using macro averaging have the same value, we aren't sure why this happened, as we used the exact same inputs to all the metrics, so we will check the performance of the models based on the F1 scoring, for different models.\n",
    "\n",
    "Observation: S1 metrics gave the best results overall, we think that adjusting the thresholds on the S2 metric can yield the same or even better results, but unfortunately we haven't the time and resources to do it by the deadline.\n",
    "\n",
    "We note that model 3 already has defined structure and the number of hidden layers doesn't affect it, so it has the same values in both graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f540b8a-9b57-4394-bf07-60a078721ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plotter(results_16hidden, \"F1\", ax=plt.subplot(131), show=False, hidden=16, legend=False)\n",
    "bar_plotter(results_16hidden, \"R\", ax=plt.subplot(132), show=False, hidden=16)\n",
    "bar_plotter(results_16hidden, \"P\", ax=plt.subplot(133), show=False, hidden=16, legend=False)\n",
    "\n",
    "plt.gcf().set_size_inches(20,5)\n",
    "plt.show()\n",
    "\n",
    "bar_plotter(results_32hidden, \"F1\", ax=plt.subplot(131), show=False, hidden=32, legend=False)\n",
    "bar_plotter(results_32hidden, \"R\", ax=plt.subplot(132), show=False, hidden=32)\n",
    "bar_plotter(results_32hidden, \"P\", ax=plt.subplot(133), show=False, hidden=32, legend=False)\n",
    "\n",
    "plt.gcf().set_size_inches(20,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd6cbc2-3488-49df-8933-098295518d30",
   "metadata": {},
   "source": [
    "## Ablation Study\n",
    "To make the study more inclusive we use the results of the F1 score since it evaluates the overall performance of the networks.\n",
    "\n",
    "### Mono Vs Stereo\n",
    "Here we plot the mono vs stereo models. \n",
    "\n",
    "We can see that stereo models outperformed the mono models when using the S1 metric while it slightly underperforms when using majority and S2 metrics. \n",
    "\n",
    "Since we are working with the fact that s1 gave better results for our thresholds we conclude that using stereo gives a better output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7363a1db-42e1-4c12-9e12-2d71194bf20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plotter(results_16hidden, \"F1\", hidden=16, model_kind=\".*[12]_stereo$|.*[12]_mono$\", ylim=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecb7897-f1f4-4648-b71e-f602a08b3316",
   "metadata": {},
   "source": [
    "### ReLU vs Leaky ReLU\n",
    "Next we examine the effect of the activation function, by comparing the results of using ReLU vs Very leaky ReLU (LReLU):\n",
    "\n",
    "We can see that they have comparable results, but using regular ReLU gave an ever so slightly better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6061a9-6ab3-4830-bcb9-836869ff5a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plotter(results_16hidden, \"F1\", hidden=16, model_kind=\"model_2_mono|model_2_stereo\", ylim=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e43e6c4-6aac-40d1-b559-bb78aa3d8a77",
   "metadata": {},
   "source": [
    "### Number of Hidden Layers\n",
    "\n",
    "We compared all versions of model 2 with different size of hidden layers i.e. 16 and 32.\n",
    "\n",
    "We can see that model 2 mono gives better F1 score when using 16 hidden layers in the Macro scoring while the results are the opposite in the Micro scoring.\n",
    "\n",
    "On the other hand the results in model 2 stereo we don't see such behavior, where the model with 16 hidden layres outperformes the 32 in s1 and s2 metrics in Macro scoring while in Micro scoring \n",
    "we see mixed behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74173cc4-2518-4d2d-bff2-adeab518f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar_plotter(results_16hidden, \"F1\", ax=plt.subplot(121), show=False, hidden=16, model_kind=\"model_2_\", ylim=0.7)\n",
    "# bar_plotter(results_32hidden, \"F1\", ax=plt.subplot(122), show=False, hidden=32, model_kind=\"model_2_\", ylim=0.7)\n",
    "# plt.gcf().set_size_inches(15,6)\n",
    "# plt.plot()\n",
    "names_16 = np.array([name+\"_16\" for name in results_16hidden.index.values], dtype=results_16hidden.index.values.dtype)\n",
    "temp_16 = results_16hidden.copy()\n",
    "temp_16 = temp_16.set_index(names_16)\n",
    "\n",
    "names_32 = np.array([name+\"_32\" for name in results_32hidden.index.values], dtype=results_32hidden.index.values.dtype)\n",
    "temp_32 = results_32hidden.copy()\n",
    "temp_32 = temp_32.set_index(names_32)\n",
    "\n",
    "concat_results = pd.concat([temp_16, temp_32])\n",
    "\n",
    "bar_plotter(concat_results, \"F1\", show=False, hidden='16 Vs. 32', model_kind=\"model_2_(mono|stereo)_[16 32]\", ylim=0.7)\n",
    "# bar_plotter(concat_results, \"F1\", ax=plt.subplot(122), show=False, hidden='16 Vs. 32', model_kind=\"model_2_stereo_[16 32]\", ylim=0.7)\n",
    "# plt.gcf().set_size_inches(15,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c56c3b-1e0a-4197-b33e-612e43b30201",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plotter(results_32hidden, \"F1\", ax=plt.subplot(131), show=False, hidden=32, model_kind=\"model_2_\", legend=False)\n",
    "bar_plotter(results_32hidden, \"P\", ax=plt.subplot(132), show=False, hidden=32, model_kind=\"model_2_\")\n",
    "bar_plotter(results_32hidden, \"R\", ax=plt.subplot(133), show=False, hidden=32, model_kind=\"model_2_\", legend=False)\n",
    "plt.gcf().set_size_inches(20,5)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7155a184-0bc2-4192-83e9-e0410c2e7c88",
   "metadata": {},
   "source": [
    "### Comparision with reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec83f9ec-3077-4c52-b979-36d283087439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(results_32hidden)\n",
    "results_model_2_lrelu = results_16hidden.loc[[\"model_1_mono\", \"model_2_stereo_lrelu\", \"model_3_mono\"]].filter(like=\"s1\")\n",
    "results_thiers = pd.DataFrame([0.51, 0.65, 0.54, 0.56, 0.52, 0.62], index=results_model_2_lrelu.columns, columns=[\"Han\"]).T\n",
    "results_model_2_lrelu = pd.concat([results_model_2_lrelu, results_thiers])\n",
    "display(results_model_2_lrelu)\n",
    "\n",
    "ax=plt.subplot(121)\n",
    "results_model_2_lrelu.filter(like=\"macro\").T.plot(\n",
    "        kind='bar',\n",
    "        ax=ax,\n",
    "        stacked=False,\n",
    "        title=\"All Scores compared with reference model\",\n",
    "        figsize=(12,6),\n",
    "        zorder=3,\n",
    "        alpha=0.9)\n",
    "ax.set_ylim(0,1)\n",
    "ax.legend(prop={'size': 8})\n",
    "ax.grid(color='k', linestyle='--', alpha=0.7, zorder=1) #ax.grid(axis='y', linestyle='--')\n",
    "ax.set_xticklabels(['F1','P\\nMacro','R'], rotation='horizontal')\n",
    "\n",
    "ax=plt.subplot(122)\n",
    "results_model_2_lrelu.filter(like=\"micro\").T.plot(\n",
    "        kind='bar',\n",
    "        ax=ax,\n",
    "        stacked=False,\n",
    "        title=\"All Scores compared with reference model\",\n",
    "        figsize=(12,6),\n",
    "        zorder=3,\n",
    "        alpha=0.9)\n",
    "ax.set_ylim(0,1)\n",
    "ax.legend(prop={'size': 8})\n",
    "ax.grid(color='k', linestyle='--', alpha=0.7, zorder=1) #ax.grid(axis='y', linestyle='--')\n",
    "ax.set_xticklabels(['F1','P\\nMicro','R'], rotation='horizontal')\n",
    "ax.get_legend().remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6f6cc0-7b76-4246-b3b7-670e2848e0ce",
   "metadata": {},
   "source": [
    "<font size=\"3\">We added table representation of the results:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee72dae-f60d-4ebe-bdd4-b12ae30c6612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_table(results, Measure): #Measure = \"F1\" or \"P\" or \"R\"\n",
    "    # prepares a table of the results for each measure i.e. F1, P or R \n",
    "    ind = results.index.values\n",
    "    df = pd.DataFrame(results.filter(regex=Measure, axis=1).to_numpy(),\n",
    "                      index=pd.Index(ind),\n",
    "                      columns=pd.MultiIndex.from_product([['Macro','Micro'],['Majority','S1','S2']], names=[Measure, 'Metric:']))\n",
    "    s = df.style.format()\n",
    "    s.set_table_styles([\n",
    "        {'selector': 'th.col_heading', 'props': 'text-align: center;'},\n",
    "        {'selector': 'th.col_heading.level0', 'props': 'font-size: 1em;'},\n",
    "        {'selector': 'td', 'props': 'text-align: center;'},\n",
    "    ], overwrite=False)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d335e7-12b1-438a-8724-400ec03b2938",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1 measure results:\")\n",
    "display(results_table(results_16hidden, \"F1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71210a55-e2a4-4e74-b08b-b1bfb6f99aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precison measure results:\")\n",
    "display(results_table(results_16hidden, \"P\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16ec8a9-1860-45a4-aa9e-ce1ac3730d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall measure results:\")\n",
    "display(results_table(results_16hidden, \"R\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da9d4c-69dd-4113-984f-95bda51b5993",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Result analysis\n",
    "\n",
    "We used 3 different models and compared the performance of each of them where we used different parameters and activation functions.\n",
    "\n",
    "## Comparison to Existing Algorithms\n",
    "Ours models achieve a slightly worse performance that the reference model in the paper, but using the coatNet architecture we can surpass the models by about 0.05 in F1 scoring, we can conclude that using a tailored network yields a better result, and we believe that the result of the paper can be further improved using RNN's alongside the CNN network, as we saw an improvement from model 1 to model 2 by adding RNN layer after the CNN network and before the fully connected layers.<br>\n",
    "Using these methods we can achieve comparable performance to attention networks which have orders of magnitude more parameters.\n",
    "\n",
    "## Effect of Activation Function\n",
    "The paper shows that suppressing the negative part of the activation rather than making it all zero improves the performance compared to normal ReLU because making the whole negative part zero might cause some initially inactive units to be never active. Moreover, it shows that using leaky ReLU, which has been proved to work well in the image classification task, also benefits the musical instrument identification. <br>\n",
    "We expected it to also improve our models but unfortunately that wasn't the case, we believe that if we played more with the thresholds of the metrics and tried different negative slope values would have achieved an improvement.\n",
    "\n",
    "## Effect of Identification Threshold\n",
    "We leave this as future work, as we didn't build the correct infrastructure to test multiple thresholds efficiently.\n",
    "\n",
    "## Effect of Channels Number\n",
    "Based on our research, the majority of the previous models use as input the mono channel, so we wanted to check wether feeding the model stereo audio would result in better recognition since there is more information in two channels compared with single channel. The results were inconclusive, as we got better performance using the s1 metric with stereo, and worse performance using the majority and s2 metrics with stereo as compared with mono.\n",
    "\n",
    "## Effect of Latent Size (Number of Hidden Layers)\n",
    "As the efffect of channels number the results were inconclusive, sometimes it impproved the performance while at other times it worsened it.<br>\n",
    "We conclude that using a smaller number of hidden layers is better since the models become smaller in size, and the performance difference is negligible.\n",
    "\n",
    "## Effect of ZCR\n",
    "We conducted an experiment, where we added ZCR information to the input data to each of the models. The ZCR as mentioned above, adds more information about the audio and was concatenated to the Mel-Spectogram replacing the last coloumn. \n",
    "The results showed that the performance is poor, we conclude that happened because the values of the ZCR $[0,1]$ are very different from the values of the mel-spectrogram $[-80,0]dB$ and so using convolutional kernels on both kinds of data mashed together gave worse results. In future work we believe that using information such as the ZCR and the Energy of the signals can improve the performance of the models as we are adding more information that the models can leverage, but they need to be treated differently than appending them as channels along side the spectogram, for example passing them through fully connected networks and combining them at fully connected layers phase of our networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7569df-a19c-4964-a4a5-c3687f977a2e",
   "metadata": {},
   "source": [
    "# Limitations\n",
    "\n",
    "\n",
    "We had some challenges during this project:\n",
    "- To begin with the data set, the training data is 3 sec audio clips, while the testing data is 5-20 sec, so we had to find an efficient way to treat and divide the test set such that it fits the architecture which is trained on 3 sec audio clips.\n",
    "- Another challenge consedring the data set is that the training data audio clips contain one instrument each where the test data contain arbitary number of instruments, which made the task multi-labeled classification so we had to come up with a way to set the threshold for the classification that considers accompanying instruments such as piano that are usualy weaker than lead instruments.\n",
    "- Training data throughput, since we had a large number of files and we applied an expensive transformation (mel_transform), most of the training time was spent on loading the data and not the actual training, to overcome this we implemented a caching option for the dataset where it caches the data after pre-processing (while considering random transforms) and works with multi-process dataloaders that require no additional configuration. One drawback is that we traded runtime performance with memory consumption, and we needed to clear the cache of the datasets at different phases to avoid filling our ram and crashing.\n",
    "- We initially planned to test our models on different metric thresholds, but we couldn't due to technical and time limitations.\n",
    "- The Precision Score with macro averaging gave the same results for all models and all metrics, we aren't sure why or what it means, but as we used a built in function to calculate it we can rule out a bad implementation. \n",
    "- The coatNet architecture didn't accept 2 channel inputs so we couldn't test the effect of mono vs stereo on that particular model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72326139-7641-446a-bdd9-4b1a177ceb43",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] J. J. Bosch, J. Janer, F. Fuhrmann, and P. Herrera, \"A Comparison of Sound Segregation Techniques for Predominant Instrument Recognition in Musical Audio Signals,\" in Proceedings of the International Society for Music Information Retrieval Conference, pp. 559-564, 2012.\n",
    "https://ismir2012.ismir.net/event/papers/559_ISMIR_2012.pdf \n",
    "\n",
    "[2] Y. Han, J. Kim, and K. Lee, \"Deep convolutional neural networks for predominant instrument recognition in polyphonic music,\" IEEE Transactions on Audio, Speech, and Language Processing, , VOL. 14, NO. 8, MAY 2016.\n",
    "https://arxiv.org/pdf/1605.09507.pdf \n",
    "\n",
    "[3] D. Szeliga, P. Tarasiuk, B. Stasiak, and P. S. Szczepaniak, \"Musical instrument recognition with a convolutional neural network and staged training,\" in Procedia Computer Science, vol. 207, pp. 2493-2502, 2022.\n",
    "https://www.sciencedirect.com/science/article/pii/S1877050922011966"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e065a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "5c9a1a0fc247903e8b21fe6ecca6675c83ea9ead6c5c7ae62a5bc3d627c779c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
